{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5847d970-1157-4d8f-b5de-fec03d78b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "import regex as re\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53560e77-462a-4343-93e3-21c6c95b76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from aistudio.google.com, provided you have a Google account\n",
    "GOOGLE_AI_STUDIO = ''  # <-- input your API key\n",
    "genai.configure(api_key=GOOGLE_AI_STUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80cfa7c4-d7b9-42f0-ade8-c31570992c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001 \t\t ['generateMessage', 'countMessageTokens']\n",
      "models/text-bison-001 \t\t ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "models/embedding-gecko-001 \t\t ['embedText', 'countTextTokens']\n",
      "models/gemini-1.0-pro-latest \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.0-pro \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-pro \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.0-pro-001 \t\t ['generateContent', 'countTokens', 'createTunedModel']\n",
      "models/gemini-1.0-pro-vision-latest \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-latest \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-001 \t\t ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-exp-0801 \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-001 \t\t ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash \t\t ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-001-tuning \t\t ['generateContent', 'countTokens', 'createTunedModel']\n",
      "models/embedding-001 \t\t ['embedContent']\n",
      "models/text-embedding-004 \t\t ['embedContent']\n",
      "models/aqa \t\t ['generateAnswer']\n"
     ]
    }
   ],
   "source": [
    "# List all available Google AI models accessible with your API key\n",
    "for m in genai.list_models():\n",
    "    print(m.name, '\\t\\t', m.supported_generation_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82004202-5c99-4823-9dd3-95cc7fb31bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-pro-exp-0801',\n",
       "    generation_config={'temperature': 0.0, 'top_p': 1, 'top_k': 1, 'max_output_tokens': 131072},\n",
       "    safety_settings={<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up model parameters\n",
    "generation_config = {\n",
    "  \"temperature\": 0.0,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 128 * 1024,   # 128k; different LLMs have different values; check model documentation\n",
    "}\n",
    "\n",
    "# All safety settings are set to None, because I don't want any censorship\n",
    "safety_settings={\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "model_name = \"gemini-1.5-pro-latest\"    # <-- this is supposedly the best Google model (but more expensive)\n",
    "model_name = \"gemini-1.5-flash-latest\"  # <-- this is a fast (but less accurate) model\n",
    "model_name = \"gemini-1.5-pro-exp-0801\"  # <-- this is currently (Aug. 2024-08) in preview mode\n",
    "\n",
    "# Enter high-level instructions (system prompt/instruction) here (and uncomment the 'system_instruction' below)\n",
    "SYSTEM_PROMPT = '''\n",
    "Recognize the following kinship terms:\n",
    "子\tson\n",
    "從兄\tcousin (father's side, same surname)\n",
    "'''\n",
    "\n",
    "model = genai.GenerativeModel(model_name=model_name,\n",
    "                              #system_instruction=SYSTEM_PROMPT,\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings\n",
    "                             )\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e42ccf-a984-4835-a363-33b30b64e661",
   "metadata": {},
   "source": [
    "### Input file to be annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dcd2ca1-2e12-4391-8f30-505ab2e33d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "fin = '/content/wudi1of3.txt'\n",
    "source_text = Path(fin).read_text(encoding='utf-8').strip().split('#####') # The delimiter ##### has been manually added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42aaf577-1d94-4492-8fc5-43813e9c7ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " '武帝上\\n\\n高祖武皇帝諱裕，字德輿，小名寄奴，彭城縣綏輿里人，[1]漢高帝弟楚元王交之後也。交生紅懿侯富，富生宗正辟彊，辟彊生陽城繆侯德，德生陽城節侯安民，安民生陽城釐侯慶忌，慶忌生陽城肅侯岑，岑生宗正平，平生東武城令某，某生東萊太守景，景生明經洽，洽生博士弘，弘生瑯邪都尉悝，悝生魏定襄太守某，某生邪城令亮，亮生晉北平太守膺，膺生相國掾熙，熙生開封令旭孫。旭孫生混，始過江，居晉陵郡丹徒縣之京口里，官至武原令。混生東安太守靖，靖生郡功曹翹，是為皇考。高祖以晉哀帝興寧元年歲次癸亥三月壬寅夜生。及長，身長七尺六寸，風骨奇特。家貧，有大志，不治廉隅。事繼母以孝謹稱。\\n\\n初為冠軍孫無終司馬。安帝隆安三年十一月，妖賊孫恩作亂於會稽，晉朝衞將軍謝琰、前將軍劉牢之東討。牢之請高祖參府軍事。十二月，牢之至吳，而賊緣道屯結，牢之命高祖與數十人覘賊遠近。會遇賊至，眾數千人，高祖便進與戰。所將人多死，而戰意方厲，手奮長刀，所殺傷甚眾。牢之子敬宣疑高祖淹久，恐為賊所困，乃輕騎尋之。既而眾騎並至，賊乃奔退，斬獲千餘人，推鋒而進，平山陰，恩遁還入海。\\n\\n四年五月，恩復入會稽，殺衞將軍謝琰。十一月，劉牢之復率眾東征，恩退走。牢之屯上虞，使高祖戍句章城。句章城既卑小，戰士不盈數百人，高祖常被堅執銳，為士卒先，每戰輒摧鋒陷陣，賊乃退還浹口。于時東伐諸帥，御軍無律，士卒暴掠，甚為百姓所苦。唯高祖法令明整，所至莫不親賴焉。\\n\\n五年春，孫恩頻攻句章，高祖屢摧破之，恩復走入海。三月，恩北出海鹽，高祖追而翼之，築城于海鹽故治。賊日來攻城，城內兵力甚弱，高祖乃選敢死之士數百人，咸脫甲冑，執短兵，並鼓噪而出，賊震懼奪氣，因其懼而奔之，並棄甲散走，斬其大帥姚盛。雖連戰剋勝，然眾寡不敵，高祖獨深慮之。一夜，偃旗匿眾，若已遁者。明晨開門，使羸疾數人登城。賊遙問劉裕所在。曰：「夜已走矣。」賊信之，乃率眾大上。高祖乘其懈怠，奮擊，大破之。恩知城不可下，乃進向滬瀆。高祖復棄城追之。海鹽令鮑陋遣子嗣之以吳兵一千，請為前驅。高祖曰：「賊兵甚精，吳人不習戰，若前驅失利，必敗我軍。可在後為聲援。」不從。是夜，高祖多設伏兵，兼置旗鼓，然一處不過數人。明日，賊率眾萬餘迎戰。前驅既交，諸伏皆出，舉旗鳴鼓。賊謂四面有軍，乃退。嗣之追奔，為賊所沒。高祖且戰且退，賊盛，所領死傷且盡。高祖慮不免，至向伏兵處，乃止，令左右脫取死人衣。賊謂當走反停，疑猶有伏。高祖因呼更戰，氣色甚猛，賊眾以為然，乃引軍去。高祖徐歸，然後散兵稍集。五月，孫恩破滬瀆，殺吳國內史袁山松，死者四千人。是月，高祖復破賊於婁縣。\\n\\n六月，恩乘勝浮海，奄至丹徒，戰士十餘萬。劉牢之猶屯山陰，京邑震動。高祖倍道兼行，與賊俱至。于時眾力既寡，加以步遠疲勞，而丹徒守軍莫有鬭志。恩率眾數萬，鼓噪登蒜山，居民皆荷擔而立。高祖率所領奔擊，大破之，投巘赴水死者甚眾。恩以彭排〈音敗〉自載，[2]僅得還船。雖被摧破，猶恃其眾力，徑向京師。樓船高大，值風不得進，旬日乃至白石。尋知劉牢之已還，朝廷有備，遂走向鬱洲。八月，以高祖為建武將軍、下邳太守，領水軍追討至鬱洲，[3]復大破恩。恩南走。十一月，高祖追恩於滬瀆，及海鹽，又破之。三戰並大獲，俘馘以萬數。恩自是饑饉疾疫，死者太半，自浹口奔臨海。\\n\\n元興元年正月，驃騎將軍司馬元顯西伐荊州刺史桓玄，玄亦率荊楚大眾，下討元顯。元顯遣鎮北將軍劉牢之拒之，高祖參其軍事。次溧洲。玄至，高祖請擊之，不許，將遣子敬宣詣玄請和。高祖與牢之甥東海何無忌並固諫，不從。[4]遂遣敬宣詣玄。玄剋京邑，殺元顯，以牢之為會稽內史。懼而告高祖曰：「便奪我兵，禍其至矣。今當北就高雅於廣陵舉事，卿能從我去乎？」答曰：「將軍以勁卒數萬，望風降服。彼新得志，威震天下。三軍人情，都已去矣，廣陵豈可得至邪！裕當反服還京口耳。」牢之叛走自縊死。何無忌謂高祖曰：「我將何之？」高祖曰：「鎮北去必不免，卿可隨我還京口。桓玄必能守節北面，我當與卿事之；不然，與卿圖之。今方是玄矯情任算之日，必將用我輩也。」桓玄從兄脩以撫軍鎮丹徒，以高祖為中兵參軍，軍、郡如故。\\n')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check no. of segments and the content of the 0th segment\n",
    "len(source_text), source_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfad19ba-7ec1-490c-9771-7b8b3aa001b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1686\n",
      "1561\n",
      "1405\n",
      "1706\n",
      "1504\n",
      "1481\n",
      "1509\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "# Check the size (no. of characters) of each segment\n",
    "for seg in source_text:\n",
    "    print(len(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "027d3adb-8fa3-41c2-8a7d-539977226906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat session\n",
    "chat = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aeafbaaf-0e67-406e-8fbe-3d38c8c8837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 7:\n",
      "\n",
      "You are an expert in the Classical Chinese language and ancient Chinese history, especially the Liu Song dynasty (5th century). You will be annotating a text enclosed in the <text> tag at the very end of this prompt. This is a multi-step prompt, consisting of 5 steps.\n",
      "\n",
      "=== STEPS BEGIN ===\n",
      "All of the following outputs from all steps will be enclosed in a single <segment number='7'> tag.\n",
      "\n",
      "Step 1:\n",
      "Please tokenize the Classical Chinese passage into word tokens (a word can be a single-character or multi-character chunk). Simply separate the tokens using a single space. Retain the punctuation marks. Retain the empty line as a visual aid. For this step output the word-tokenized text within the tag <step1>.\n",
      "\n",
      "Step 2:\n",
      "Using the output from Step 1 (tokenized text), enclosed each word token with the XML-like tag <ne> (for named entity) only if the token is a named entity (NE), such as personal name, geographic location, date, job title, etc. Do not use a tag for a non-NE. For this step output the annotated text within the tag <step2>. \n",
      "\n",
      "Step 3:\n",
      "Using the output from Step 2, for each identified named entity within the <ne> tag, replace the <ne> tag with one of the following specific tags based on the named entity's classification:\n",
      "<p> for a personal name;\n",
      "<g> for a geographical name;\n",
      "<o> for the name of a political office or job title;\n",
      "<et> for an emperor's temple name 廟號 (e.g., 高祖);\n",
      "<ep> for an emperor's posthumous name 諡號 (e.g., 武帝);\n",
      "<era> for an imperial era name (e.g., 永初);\n",
      "<dy> for a dynastic name (e.g., 晉, 漢);\n",
      "<k> for a kinship term, such as 父, 母, 子, 女, 甥, 繼母, 從弟, 從兄, 從子, 兄子, 弟子, 伯, 叔, 舅, 祖父;\n",
      "<d> for a date or time.\n",
      "For this step output the annotated text within the tag <step3>.\n",
      "\n",
      "Step 4:\n",
      "Use the the output from Step 3, for each named entity identified as a personal name (<p>), attempt to recover the full name associated with this named entity by adding the surname; place this full name as an attribute to the <p> tag. For example, <p fn=\"劉裕\">裕<p/>, who is the founding emperor of the Liu Song Dynasty 劉裕 with the surnamed 劉. Output the entire annotated text. For a named entity identified as a date, if it is a year, attempt to prefix it with the proper era name, and place this full year designation (e.g., 永初二年, 隆安三年) as an attribute to the <d> tag, e.g., <d fd='永初二年'>二年</d>. Output the annotated text within the tag <step4>.\n",
      "\n",
      "Step 5:\n",
      "Provide a summary of the annotations made at the end within a <summary> tag.\n",
      "\n",
      "=== STEPS END ===\n",
      "\n",
      "The text to be annotated is:\n",
      "<text>\n",
      "七月庚申，羣賊自蔡洲南走，還屯尋陽。遣輔國將軍王仲德、廣川太守劉鍾、河間太守蒯恩追之。公還東府，大治水軍，皆大艦重樓，高者十餘丈。盧循遣其大將荀林寇江陵，[28]桓謙先於江陵奔羌，又自羌入蜀，偽主譙縱以為荊州刺史。謙及譙道福率軍二萬，出寇江陵，適與林會，相去百餘里。荊州刺史道規斬謙于枝江，破林於江津，追至竹町斬之。\n",
      "\n",
      "初循之走也，公知其必寇江陵，登遣淮陵內史索邈領馬軍步道援荊州。又遣建威將軍孫季高率眾三千，自海道襲番禺。江州刺史庾悅至五畝嶠，賊遣千餘人據斷嶠道，悅前驅鄱陽太守虞丘進攻破之。公治兵大辦。十月，率兗州刺史劉藩、寧朔將軍檀韶等舟師南伐。以後將軍劉毅監太尉留守府，後事皆委焉。\n",
      "\n",
      "是月，徐道覆率眾三萬寇江陵。荊州刺史道規又大破之，斬首萬餘級，道覆走還盆口。初公之遣索邈也，邈在道為賊所斷，道覆敗後方達。自循東下，江陵斷絕京邑之問，傳者皆云已沒。及邈至，方知循走。\n",
      "\n",
      "循初自蔡洲南走，留其親黨范崇民五千人，高艦百餘，戍南陵。王仲德等聞大軍且至，乃進攻之。十一月，大破崇民軍，焚其舟艦，收其散卒。\n",
      "\n",
      "循廣州守兵，不以海道為防。是月，建威將軍孫季高乘海奄至，而城池峻整，兵猶數千。季高焚賊舟艦，悉力而上，四面攻之，即日屠其城。循父以輕舟奔始興。季高撫其舊民，戮其親黨，勒兵謹守。初公之遣季高也，眾咸以海道艱遠，必至為難；且分撤見力，二三非要。公不從。敕季高曰：「大軍十二月之交，必破妖虜。卿今時當至廣州，傾其巢窟，令賊奔走之日，無所歸投。」季高受命而行，如期剋捷。\n",
      "\n",
      "循方治兵旅舟艦，設諸攻備。公欲御以長算，乃屯軍雷池。賊揚聲不攻雷池，當乘流逕下。公知其欲戰，且慮賊戰敗，或於京江入海，遣王仲德以水艦二百於吉陽下斷之。十二月，循、道覆率眾數萬，方艦而下，前後相抗，莫見舳艫之際。公悉出輕利鬭艦，躬提幡鼓，命眾軍齊力擊之。又上步騎於西岸。右軍參軍庾樂生乘艦不進，斬而徇之。於是眾軍並踊騰爭先。軍中多萬鈞神弩，所至莫不摧陷。公中流蹙之，因風水之勢，賊艦悉泊西岸。岸上軍先備火具，[29]乃投火焚之，煙爓張天，賊眾大敗，追奔至夜乃歸。循等還尋陽。初分遣步軍，莫不疑怪，及燒賊艦，眾乃悅服。召王仲德，請還為前驅。留輔國將軍孟懷玉守雷池。循聞大軍上，欲走向豫章，乃悉力柵斷左里。大軍至左里，將戰，公所執麾竿折，折幡沈水，眾並怪懼。公歡笑曰：「往年覆舟之戰，幡竿亦折，今者復然，賊必破矣。」即攻柵而進。遁兵雖殊死戰，弗能禁。諸軍乘勝奔之，循單舸走。所殺及投水死，凡萬餘人。納其降附，宥其逼略。遣劉藩、孟懷玉輕軍追之。循收散卒，尚有數千人，逕還廣州。道覆還保始興。公旋自左里。天子遣侍中、黃門勞師于行所。\n",
      "</text>\n",
      "\n",
      "Elapsed time for Segment 7: 183.53 seconds\n",
      "=========================\n",
      "**** Sleeping 30 seconds....\n",
      "**** Now continuing...\n",
      "\n",
      "\n",
      "***** ALL DONE!!!\n"
     ]
    }
   ],
   "source": [
    "# Removed \"For a date-like (\"era name + year + month\", or \"era name + year\", or \"year + month\") character sequence (e.g., 隆安三年二月, 五年七月), treat the sequence as a single token. \" from Step 1 - 2024-08-21\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an expert in the Classical Chinese language and ancient Chinese history, especially the Liu Song dynasty (5th century). You will be annotating a text enclosed in the <text> tag at the very end of this prompt. This is a multi-step prompt, consisting of 5 steps.\n",
    "\n",
    "=== STEPS BEGIN ===\n",
    "All of the following outputs from all steps will be enclosed in a single <segment number='{idx}'> tag.\n",
    "\n",
    "Step 1:\n",
    "Please tokenize the Classical Chinese passage into word tokens (a word can be a single-character or multi-character chunk). Simply separate the tokens using a single space. Retain the punctuation marks. Retain the empty line as a visual aid. For this step output the word-tokenized text within the tag <step1>.\n",
    "\n",
    "Step 2:\n",
    "Using the output from Step 1 (tokenized text), enclosed each word token with the XML-like tag <ne> (for named entity) only if the token is a named entity (NE), such as personal name, geographic location, date, job title, etc. Do not use a tag for a non-NE. For this step output the annotated text within the tag <step2>. \n",
    "\n",
    "Step 3:\n",
    "Using the output from Step 2, for each identified named entity within the <ne> tag, replace the <ne> tag with one of the following specific tags based on the named entity's classification:\n",
    "<p> for a personal name;\n",
    "<g> for a geographical name;\n",
    "<o> for the name of a political office or job title;\n",
    "<et> for an emperor's temple name 廟號 (e.g., 高祖);\n",
    "<ep> for an emperor's posthumous name 諡號 (e.g., 武帝);\n",
    "<era> for an imperial era name (e.g., 永初);\n",
    "<dy> for a dynastic name (e.g., 晉, 漢);\n",
    "<k> for a kinship term, such as 父, 母, 子, 女, 甥, 繼母, 從弟, 從兄, 從子, 兄子, 弟子, 伯, 叔, 舅, 祖父;\n",
    "<d> for a date or time.\n",
    "For this step output the annotated text within the tag <step3>.\n",
    "\n",
    "Step 4:\n",
    "Use the the output from Step 3, for each named entity identified as a personal name (<p>), attempt to recover the full name associated with this named entity by adding the surname; place this full name as an attribute to the <p> tag. For example, <p fn=\"劉裕\">裕<p/>, who is the founding emperor of the Liu Song Dynasty 劉裕 with the surnamed 劉. Output the entire annotated text. For a named entity identified as a date, if it is a year, attempt to prefix it with the proper era name, and place this full year designation (e.g., 永初二年, 隆安三年) as an attribute to the <d> tag, e.g., <d fd='永初二年'>二年</d>. Output the annotated text within the tag <step4>.\n",
    "\n",
    "Step 5:\n",
    "Provide a summary of the annotations made at the end within a <summary> tag.\n",
    "\n",
    "=== STEPS END ===\n",
    "\n",
    "The text to be annotated is:\n",
    "<text>\n",
    "{text_to_be_annotated}\n",
    "</text>\n",
    "\"\"\"\n",
    "\n",
    "SLEEP_TIME = 30\n",
    "\n",
    "fon = '/content/wudi1of3_annotated.Gemini1.5Pro.Exp0801.txt'\n",
    "\n",
    "with open(fon, 'a', encoding='utf-8', newline='\\n') as fo:\n",
    "\n",
    "    for idx, src in enumerate(source_text[0:]):\n",
    "        #if idx < 7: continue\n",
    "        if src.startswith('#'): continue  # skip the #### lines\n",
    "        text_to_be_annotated = src.strip()\n",
    "\n",
    "        prompt = PROMPT.format(text_to_be_annotated=text_to_be_annotated, idx=idx)\n",
    "        print(f'Segment {idx}:') \n",
    "        print(prompt)\n",
    "\n",
    "        if idx % 2 == 0:  # clear history for every two chat.send_message() calls \n",
    "            chat = model.start_chat(history=[])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = chat.send_message(prompt)\n",
    "        for chunk in response:\n",
    "            fo.write(chunk.text + '\\n')\n",
    "            #fo.write(\"_\"*80 + '\\n')\n",
    "        fo.flush()\n",
    "        \n",
    "        response = chat.send_message('Please continue.')  # if there are too many output tokens, we might need this prompt to force the model to complete the text-generation process\n",
    "        for chunk in response:\n",
    "            fo.write(chunk.text + '\\n')\n",
    "            fo.write(\"_\"*80 + '\\n')\n",
    "        fo.flush()\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time for Segment {idx}: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        print('='*25)\n",
    "        print(f'**** Sleeping {SLEEP_TIME} seconds....')\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        print('**** Now continuing...')\n",
    "\n",
    "print(\"\\n\\n***** ALL DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e76bef-f9e4-422b-9679-02a2ff053d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatSession(\n",
       "    model=genai.GenerativeModel(\n",
       "        model_name='models/gemini-1.5-pro-exp-0801',\n",
       "        generation_config={'temperature': 0.0, 'top_p': 1, 'top_k': 1, 'max_output_tokens': 131072},\n",
       "        safety_settings={<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>},\n",
       "        tools=None,\n",
       "        system_instruction=\"\\nRecognize the following kinship terms:\\n子\\tson\\n從兄\\tcousin (father's side, same surname)\\n\",\n",
       "        cached_content=None\n",
       "    ),\n",
       "    history=[protos.Content({'parts': [{'text': '\\nYou are an...故。\\n</text>\\n'}], 'role': 'user'}), protos.Content({'parts': [{'text': '```xml\\n<seg...東海</g> <p>何無忌'}], 'role': 'model'}), protos.Content({'parts': [{'text': '\\nYou are an...故。\\n</text>\\n'}], 'role': 'user'}), protos.Content({'parts': [{'text': '```xml\\n<seg...東海</g> <p>何無忌'}], 'role': 'model'}), protos.Content({'parts': [{'text': 'Please continue.'}], 'role': 'user'}), protos.Content({'parts': [{'text': '```xml\\n並 固 ...segment>\\n```'}], 'role': 'model'})]\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9d729-20fe-4f1d-99db-43cd362d6b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
